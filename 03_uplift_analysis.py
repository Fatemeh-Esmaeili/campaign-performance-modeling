# -*- coding: utf-8 -*-
"""03_uplift_analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1L2EyQpGqE12LRpCpIeDzRdjAnH2a2hJM

# Uplift Analysis & A/B Testing

## 1- Introduction - Uplift Analysis & Campaign Effectiveness

This notebook evaluates the impact of a marketing campaign using A/B testing and uplift modeling techniques. We'll quantify how much the campaign changed customer behavior and identify which customer segments responded positively.

We'll answer questions such as:
- Did the campaign increase customer response rates?
- Which groups benefitted most from the campaign?
- Can we model uplift at the customer level?

üí° **What is Uplift?**

Uplift is the difference in behavior (like conversion or response) between people who received a treatment (e.g., a marketing campaign) and those who didn‚Äôt (control group). It tells you the incremental effect of the campaign ‚Äî the true added value.

 üí° **In simple terms:**

$$ \text{Uplift} = \text{Response Rate (Treatment)} ‚àí \text{Response Rate (Control)} $$ $$ =  \text{P(Response | Treatment=1)} ‚àí  \text{P(Response | Treatment=0)}$$

It measures how much more likely someone is to take the desired action because of the campaign.

üí° **What Is Uplift Modeling?**

Uplift modeling is a technique used to predict the incremental impact of a treatment (like a marketing campaign) on an individual‚Äôs behavior ‚Äî compared to if they hadn‚Äôt received the treatment.


üí°**Key Idea:**

Instead of predicting **whether** a person will respond, **uplift models** predict **how much more likely** a person is to respond **because of** the treatment.

üí° **Why Not Just Use a Classifier?**
* A traditional classifier predicts:

      ‚ÄúWho is likely to respond?‚Äù

* But uplift modeling asks:

      ‚ÄúWho is likely to respond because of the campaign?‚Äù

This allows marketers to:

* Avoid targeting customers who would buy anyway.

* Avoid annoying customers who would have churned due to the ad.

* Focus spend on the persuadable ones.
"""

from google.colab import files
uploaded = files.upload()

""" ## 2- Load Data"""

# !pip install xgboost lightgbm

# !pip install causalml

# 2. Import Libraries

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from causalml.inference.tree import UpliftTreeClassifier
from causalml.metrics import plot_gain
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

# 3. Load Cleaned Data

import pandas as pd

# Load the dataset
df = pd.read_csv('campaign_data_cleaned.csv')

# Preview
df.head()

"""## 3. Exploratory Check: Campaign vs. Control

Analyzing how many people responded to the marketing campaign, depending on whether they received the campaign or not.

Data in column **['received_campaign']** are splitted into two groups:

*   **0: Control group** (did not receive campaign)
*   **1: Treatment group** (did receive campaign)
"""

df.groupby('received_campaign')['responded'].value_counts()

# 4. Basic A/B Testing ‚Äì Response Rate

ab_summary = df.groupby("received_campaign")["responded"].agg(['count', 'sum', 'mean']).rename(columns={'mean': 'mean =response_rate'})
ab_summary.index = ['Control (0)', 'Treatment (1)']
ab_summary

"""**Column meanings:**

* **Count:** Number of customers in each group.

* **Sum:** Number of customers who responded (responded = 1) in that group.

* **Mean (Response Rate):** The proportion of responders in each group, calculated as sum / count.

**Interpretation:**



*   Control Group (0):

    *   These customers **did not receive** the campaign.
    *   Only ~4.8% of them responded, likely due to natural engagement or randomness.


*  Treatment Group (1):

    *   These customers **did receive** the campaign.
    *   ~16.4% responded, which is much higher than the control group.
"""

# Visualize response rates

sns.barplot(x=df['received_campaign'], y=df['responded'])
plt.title("Response Rate by Campaign Group")
plt.xticks([0, 1], ['Control', 'Treatment'])
plt.ylabel("Response Rate")
plt.show()

"""That Chi-squared test p-value of 1.45e-40 is extremely small; $ p - {value} < 0.05 $, which means:

‚úÖ **Interpretation:**


* Null Hypothesis (H‚ÇÄ): There is no association between receiving the campaign and responding.

* Alternative Hypothesis (H‚ÇÅ): There is an association between receiving the campaign and responding.

üîç Conclusion:
Since the *p-value* is much smaller than any reasonable alpha level (e.g., 0.05, 0.01, or even 0.0001):



‚úÖ Reject the null hypothesis.

**This means the difference in response rates is not due to random chance ‚Äî the campaign likely had a real effect on customer behavior.**


"""

# 5. Calculate Uplift (Simple Difference in Means)

treatment_rate = df[df['received_campaign'] == 1]['responded'].mean()
control_rate = df[df['received_campaign'] == 0]['responded'].mean()
uplift = treatment_rate - control_rate

print(f"Control Response Rate: {control_rate:.4f}")
print(f"Treatment Response Rate: {treatment_rate:.4f}")
print(f"Estimated (Overall) Uplift: {uplift:.4f}")

"""**This means the campaign increased the response rate by 11.6 percentage points.**

The difference between the two response rates (~16.4% - 4.8% = 11.6%) is the uplift ‚Äî the estimated causal effect of the campaign on customer response.

# Calculating p-value

Just comparing response rates is not enough. To determine whether the difference between treatment and control is statistically significant and not due to random chance, we should calculate a p-value.


A p-value helps you answer:

"Assuming there is no real effect, what is the probability of seeing a difference this large (or larger) just by chance?"


* **Null Hypothesis (H‚ÇÄ):** There is no association between receiving the campaign and responding.

* **Alternative Hypothesis (H‚ÇÅ):** There is an association between receiving the campaign and responding.



If the p-value is small (typically < 0.05), you can reject the null hypothesis (that the campaign had no effect) and say the campaign likely caused the increase in response.
"""

# Calculate p-value

from scipy.stats import chi2_contingency

# Create a contingency table
# Rows = treatment group (0 = control, 1 = treatment)
# Columns = response (0 = no, 1 = yes)
contingency_table = pd.crosstab(df['received_campaign'], df['responded'])

# Perform chi-squared test
chi2, p, dof, expected = chi2_contingency(contingency_table)

print("Chi-squared test p-value:", p)

"""# 6. Prepare Features for Uplift Modelings

Preparing data to train an uplift model that answers:

    ‚ÄúGiven this customer‚Äôs features, what‚Äôs the probability they respond if treated vs if not treated?‚Äù
"""

features = ['age', 'income', 'tenure_years', 'credit_score', 'is_high_value', 'spend_last_month']

# Customer features
X = df[features]

# Whether they received the campaign
treatment = df['received_campaign']

# Whether they responded
y = df['responded']


df[features].describe()

"""# A - Uplift Tree Model - causalml Library"""

from causalml.inference.tree import UpliftTreeClassifier

# Convert your treatment column to string labels

df['treatment_str'] = df['received_campaign'].map({0: 'control', 1: 'treatment'})
df['treatment_str']



uplift_model = UpliftTreeClassifier(
    control_name='control',
    max_depth=10,
    min_samples_leaf=50,
    min_samples_treatment=30,
    n_reg=10
)


uplift_model.fit(X=X.values, treatment=df['treatment_str'].values, y=y.values)

# Predict uplift scores - this returns a 2D array with shape (n_samples, 2)
uplift_preds = uplift_model.predict(X.values)

# Calculate uplift = P(response | treatment) - P(response | control)
uplift_scores = uplift_preds[:, 1] - uplift_preds[:, 0]

# Save to the DataFrame
df['uplift_score'] = uplift_scores

# Now this will work fine
df['uplift_score']
df.head()

# 8. Plot Uplift Gain Curve

from causalml.metrics import plot_gain
import matplotlib.pyplot as plt

# Ensure the DataFrame is correctly created
df_uplift = pd.DataFrame({
    'treatment': df['received_campaign'],
    'outcome': df['responded'],                      # 0 or 1
    'uplift_score': uplift_scores      # predicted uplift score (1D array)
})


print(df_uplift.head())


# To break ties in uplift scores:
df_uplift['uplift_score_noisy'] = df_uplift['uplift_score'] + np.random.normal(0, 1e-5, size=len(df_uplift))

plot_gain(
    df_uplift,
    outcome_col='outcome',
    treatment_col='treatment',
    treatment_effect_col='uplift_score_noisy'
)
plt.title("Uplift Gain Curve (With Small Noise)")
plt.show()

from causalml.metrics import plot_qini

plot_qini(
    df_uplift,
    outcome_col='outcome',
    treatment_col='treatment',
    treatment_effect_col='uplift_score'
)
plt.title("Qini Curve")
plt.show()

# 9. Top Target Group

# Target top 10% with highest uplift score
top_10 = df.sort_values("uplift_score", ascending=False).head(int(0.1 * len(df)))
#top_10.describe()
top_10.head()

# 10. Save Outputs

df.to_csv("/content/sample_data/campaign_data_with_uplift.csv", index=False)

"""# B - T-Learner - Uplift Tree Model - Two Model Approach- sklearn Library

## Fit the model
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

# Create treatment and control datasets
features = ['age', 'income', 'tenure_years', 'credit_score', 'is_high_value', 'spend_last_month']
X = df[features]
y = df['responded']
t = df['received_campaign']

# Split df
X_train, X_test, y_train, y_test, t_train, t_test = train_test_split(
    X, y, t, test_size=0.3, random_state=42
)

# Train two separate models
model_treat = RandomForestClassifier(random_state=42)
model_control = RandomForestClassifier(random_state=42)

model_treat.fit(X_train[t_train == 1], y_train[t_train == 1])
model_control.fit(X_train[t_train == 0], y_train[t_train == 0])

# Predict uplift
pred_treat = model_treat.predict_proba(X_test)[:, 1]
pred_control = model_control.predict_proba(X_test)[:, 1]
uplift_score_RF = pred_treat - pred_control

# Add predictions to DataFrame
uplift_df = X_test.copy()
uplift_df['uplift_RF'] = uplift_score_RF
#uplift_df['actual_response'] = y_test.reset_index(drop=True)
#uplift_df['treatment'] = t_test.reset_index(drop=True)


uplift_df['actual_response_RF'] = y_test
uplift_df['treatment_RF'] = t_test
uplift_df.head()

"""## Visualizing Uplift by Segment
or

Or group by a customer segment:
"""

import matplotlib.pyplot as plt
import seaborn as sns

# Visualize uplift distribution
sns.histplot(uplift_score_RF, kde=True)
plt.title('Uplift Score Distribution')
plt.xlabel('Predicted Uplift')
plt.show()

"""group by a customer segment:"""

segment_uplift = df.groupby('customer_segment').apply(
    lambda df: df[df['received_campaign']==1]['responded'].mean() -
               df[df['received_campaign']==0]['responded'].mean()
).reset_index(name='segment_lift')

sns.barplot(data=segment_uplift, x='customer_segment', y='segment_lift')
plt.title('Lift by Customer Segment')

"""## Interpretation

**Segment 1** has the highest uplift (~0.14):

* Customers in this segment responded 14 percentage points more when they received the campaign compared to when they didn‚Äôt.

* Ideal target for future campaigns.

**Segments 0 and 2 have** moderate lift (~0.12):

* The campaign was also effective for them.

* Consider continuing or refining targeting strategies here.

**Segment 3** has the lowest lift (~0.09):

* The campaign still had a positive effect, but not as strong.

* You might want to investigate why (e.g., smaller sample size, already high base response rate, saturation).

## Calculate Uplift and Standard Error per Segment
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import sem

# Function to compute lift and its standard error
def compute_segment_lift(group):
    treat = group[group['received_campaign'] == 1]['responded']
    control = group[group['received_campaign'] == 0]['responded']

    lift = treat.mean() - control.mean()

    # Standard error = sqrt(se_treat¬≤ + se_control¬≤)
    se = np.sqrt(sem(treat, nan_policy='omit')**2 + sem(control, nan_policy='omit')**2)

    return pd.Series({'segment_lift': lift, 'se': se})

# Apply per segment
segment_stats = df.groupby('customer_segment').apply(compute_segment_lift).reset_index()

"""## Plot with Error Bars Using Seaborn"""

sns.barplot(
    data=segment_stats,
    x='customer_segment',
    y='segment_lift',
    yerr=segment_stats['se'],       # error bars
    capsize=0.1,                    # small cap on error bars
    color='skyblue'
)

plt.title('Lift by Customer Segment with Error Bars')
plt.ylabel('Segment Uplift (Treatment - Control)')
plt.xlabel('Customer Segment')
plt.tight_layout()
plt.show()

"""- Overall campaign lift was 11.6%.
- Certain customer segments 1 showed higher positive response.
- Uplift modeling helps prioritize customers who are more likely to respond due to the campaign.

## Next Step

Proceed to customer segmentation in `02_customer_segmentation.ipynb` to explore different customer clusters and improve targeting precision.
"""